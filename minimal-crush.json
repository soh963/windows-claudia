{
    "mcp":  {
                "filesystem":  {
                                   "args":  [
                                                "D:\\",
                                                "C:\\"
                                            ],
                                   "command":  "@modelcontextprotocol/server-filesystem",
                                   "type":  "stdio"
                               }
            },
    "providers":  {
                      "ollama":  {
                                     "id":  "ollama",
                                     "name":  "Ollama Local",
                                     "models":  [
                                                    {
                                                        "name":  "Llama 3.1 8B",
                                                        "id":  "llama3.1:8b",
                                                        "max_tokens":  8192
                                                    }
                                                ],
                                     "api_key":  "ollama",
                                     "type":  "openai",
                                     "base_url":  "http://localhost:11434/v1"
                                 }
                  }
}
