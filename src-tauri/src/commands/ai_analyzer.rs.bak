use anyhow::Result;
use log::{info, warn};
use rusqlite::{params, Connection};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::fs;
use std::path::Path;
use std::time::{SystemTime, UNIX_EPOCH};
use tauri::State;

use super::agents::AgentDb;
use super::dashboard::AIUsageMetric;

/// AI Usage Analysis Result
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct AIUsageAnalysis {
    pub project_id: String,
    pub total_sessions: i32,
    pub total_tokens: i64,
    pub total_requests: i64,
    pub average_success_rate: f64,
    pub cost_analysis: CostAnalysis,
    pub model_breakdown: Vec<ModelUsage>,
    pub agent_breakdown: Vec<AgentUsage>,
    pub mcp_server_usage: Vec<MCPServerUsage>,
    pub efficiency_metrics: EfficiencyMetrics,
    pub usage_patterns: UsagePatterns,
    pub recommendations: Vec<String>,
}

/// Cost Analysis
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct CostAnalysis {
    pub total_cost: f64,
    pub daily_average: f64,
    pub cost_per_token: f64,
    pub cost_breakdown_by_model: HashMap<String, f64>,
    pub projected_monthly_cost: f64,
    pub cost_efficiency_score: f64,
}

/// Model Usage Statistics
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ModelUsage {
    pub model_name: String,
    pub total_tokens: i64,
    pub total_requests: i64,
    pub success_rate: f64,
    pub average_response_time: i64,
    pub cost: f64,
    pub usage_percentage: f64,
    pub efficiency_score: f64,
}

/// Agent Usage Statistics
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct AgentUsage {
    pub agent_type: String,
    pub total_tokens: i64,
    pub total_requests: i64,
    pub success_rate: f64,
    pub specialization_score: f64,
    pub performance_score: f64,
    pub usage_frequency: f64,
}

/// MCP Server Usage Statistics
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct MCPServerUsage {
    pub server_name: String,
    pub total_requests: i64,
    pub success_rate: f64,
    pub average_response_time: i64,
    pub reliability_score: f64,
    pub integration_efficiency: f64,
}

/// Efficiency Metrics
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct EfficiencyMetrics {
    pub tokens_per_successful_request: f64,
    pub request_completion_rate: f64,
    pub average_session_duration: i64,
    pub productivity_score: f64,
    pub resource_utilization: f64,
}

/// Usage Patterns
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct UsagePatterns {
    pub peak_usage_hours: Vec<i32>,
    pub most_used_agents: Vec<String>,
    pub most_used_models: Vec<String>,
    pub session_length_distribution: HashMap<String, i32>,
    pub failure_patterns: Vec<String>,
}

/// Real-time AI usage analyzer
#[tauri::command]
pub async fn analyze_ai_usage(
    db: State<'_, AgentDb>,
    project_id: String,
    days_back: Option<i64>,
) -> Result<AIUsageAnalysis, String> {
    info!("Analyzing AI usage patterns for project: {}", project_id);
    
    let conn = db.0.lock().map_err(|e| e.to_string())?;
    let days = days_back.unwrap_or(30);
    
    // Get AI usage metrics from database
    let metrics = get_ai_usage_metrics(&conn, &project_id, Some(days))?;
    
    if metrics.is_empty() {
        return Ok(create_empty_analysis(project_id));
    }
    
    // Analyze current usage data
    let total_sessions = metrics.len() as i32;
    let total_tokens: i64 = metrics.iter().map(|m| m.token_count).sum();
    let total_requests: i64 = metrics.iter().map(|m| m.request_count).sum();
    let total_successes: i64 = metrics.iter().map(|m| m.success_count).sum();
    let average_success_rate = if total_requests > 0 {
        (total_successes as f64 / total_requests as f64) * 100.0
    } else { 0.0 };
    
    // Analyze costs
    let cost_analysis = analyze_costs(&metrics, days);
    
    // Model breakdown
    let model_breakdown = analyze_model_usage(&metrics);
    
    // Agent breakdown
    let agent_breakdown = analyze_agent_usage(&metrics);
    
    // MCP server usage
    let mcp_server_usage = analyze_mcp_usage(&metrics);
    
    // Efficiency metrics
    let efficiency_metrics = calculate_efficiency_metrics(&metrics);
    
    // Usage patterns
    let usage_patterns = analyze_usage_patterns(&metrics);
    
    // Generate recommendations
    let recommendations = generate_ai_recommendations(&metrics, &cost_analysis, &efficiency_metrics);
    
    // Collect real-time Claude Code session data
    collect_realtime_ai_data(&conn, &project_id).await?;
    
    Ok(AIUsageAnalysis {
        project_id,
        total_sessions,
        total_tokens,
        total_requests,
        average_success_rate,
        cost_analysis,
        model_breakdown,
        agent_breakdown,
        mcp_server_usage,
        efficiency_metrics,
        usage_patterns,
        recommendations,
    })
}

/// Analyze cost patterns
fn analyze_costs(metrics: &[AIUsageMetric], days: i64) -> CostAnalysis {
    let total_cost: f64 = metrics.iter().map(|m| m.total_cost.unwrap_or(0.0)).sum();
    let total_tokens: i64 = metrics.iter().map(|m| m.token_count).sum();
    
    let daily_average = total_cost / days as f64;
    let cost_per_token = if total_tokens > 0 {
        total_cost / total_tokens as f64
    } else { 0.0 };
    
    // Cost breakdown by model
    let mut cost_breakdown_by_model = HashMap::new();
    for metric in metrics {
        let cost = metric.total_cost.unwrap_or(0.0);
        *cost_breakdown_by_model.entry(metric.model_name.clone()).or_insert(0.0) += cost;
    }
    
    let projected_monthly_cost = daily_average * 30.0;
    
    // Calculate cost efficiency score
    let cost_efficiency_score = calculate_cost_efficiency(metrics);
    
    CostAnalysis {
        total_cost,
        daily_average,
        cost_per_token,
        cost_breakdown_by_model,
        projected_monthly_cost,
        cost_efficiency_score,
    }
}

/// Analyze model usage patterns
fn analyze_model_usage(metrics: &[AIUsageMetric]) -> Vec<ModelUsage> {
    let mut model_stats: HashMap<String, (i64, i64, i64, i64, f64, i64)> = HashMap::new();
    
    let total_tokens: i64 = metrics.iter().map(|m| m.token_count).sum();
    
    for metric in metrics {
        let entry = model_stats.entry(metric.model_name.clone()).or_insert((0, 0, 0, 0, 0.0, 0));
        entry.0 += metric.token_count;
        entry.1 += metric.request_count;
        entry.2 += metric.success_count;
        entry.3 += metric.failure_count;
        entry.4 += metric.total_cost.unwrap_or(0.0);
        entry.5 += metric.avg_response_time.unwrap_or(0);
    }
    
    model_stats.into_iter().map(|(model_name, (tokens, requests, successes, failures, cost, response_time))| {
        let success_rate = if requests > 0 {
            (successes as f64 / requests as f64) * 100.0
        } else { 0.0 };
        
        let usage_percentage = if total_tokens > 0 {
            (tokens as f64 / total_tokens as f64) * 100.0
        } else { 0.0 };
        
        let average_response_time = if requests > 0 {
            response_time / requests
        } else { 0 };
        
        let efficiency_score = calculate_model_efficiency(success_rate, average_response_time, tokens, cost);
        
        ModelUsage {
            model_name,
            total_tokens: tokens,
            total_requests: requests,
            success_rate,
            average_response_time,
            cost,
            usage_percentage,
            efficiency_score,
        }
    }).collect()
}

/// Analyze agent usage patterns
fn analyze_agent_usage(metrics: &[AIUsageMetric]) -> Vec<AgentUsage> {
    let mut agent_stats: HashMap<String, (i64, i64, i64, i64)> = HashMap::new();
    
    for metric in metrics {
        if let Some(agent_type) = &metric.agent_type {
            let entry = agent_stats.entry(agent_type.clone()).or_insert((0, 0, 0, 0));
            entry.0 += metric.token_count;
            entry.1 += metric.request_count;
            entry.2 += metric.success_count;
            entry.3 += metric.failure_count;
        }
    }
    
    let total_requests: i64 = agent_stats.values().map(|(_, requests, _, _)| *requests).sum();
    
    agent_stats.into_iter().map(|(agent_type, (tokens, requests, successes, failures))| {
        let success_rate = if requests > 0 {
            (successes as f64 / requests as f64) * 100.0
        } else { 0.0 };
        
        let usage_frequency = if total_requests > 0 {
            (requests as f64 / total_requests as f64) * 100.0
        } else { 0.0 };
        
        let specialization_score = calculate_agent_specialization(&agent_type, tokens, success_rate);
        let performance_score = calculate_agent_performance(success_rate, tokens, requests);
        
        AgentUsage {
            agent_type,
            total_tokens: tokens,
            total_requests: requests,
            success_rate,
            specialization_score,
            performance_score,
            usage_frequency,
        }
    }).collect()
}

/// Analyze MCP server usage
fn analyze_mcp_usage(metrics: &[AIUsageMetric]) -> Vec<MCPServerUsage> {
    let mut mcp_stats: HashMap<String, (i64, i64, i64, i64)> = HashMap::new();
    
    for metric in metrics {
        if let Some(mcp_server) = &metric.mcp_server {
            let entry = mcp_stats.entry(mcp_server.clone()).or_insert((0, 0, 0, 0));
            entry.0 += metric.request_count;
            entry.1 += metric.success_count;
            entry.2 += metric.failure_count;
            entry.3 += metric.avg_response_time.unwrap_or(0);
        }
    }
    
    mcp_stats.into_iter().map(|(server_name, (requests, successes, failures, response_time))| {
        let success_rate = if requests > 0 {
            (successes as f64 / requests as f64) * 100.0
        } else { 0.0 };
        
        let average_response_time = if requests > 0 {
            response_time / requests
        } else { 0 };
        
        let reliability_score = calculate_mcp_reliability(success_rate, average_response_time);
        let integration_efficiency = calculate_mcp_efficiency(&server_name, success_rate, requests);
        
        MCPServerUsage {
            server_name,
            total_requests: requests,
            success_rate,
            average_response_time,
            reliability_score,
            integration_efficiency,
        }
    }).collect()
}

/// Calculate efficiency metrics
fn calculate_efficiency_metrics(metrics: &[AIUsageMetric]) -> EfficiencyMetrics {
    let total_tokens: i64 = metrics.iter().map(|m| m.token_count).sum();
    let total_requests: i64 = metrics.iter().map(|m| m.request_count).sum();
    let total_successes: i64 = metrics.iter().map(|m| m.success_count).sum();
    
    let tokens_per_successful_request = if total_successes > 0 {
        total_tokens as f64 / total_successes as f64
    } else { 0.0 };
    
    let request_completion_rate = if total_requests > 0 {
        (total_successes as f64 / total_requests as f64) * 100.0
    } else { 0.0 };
    
    // Estimate session duration (simplified)
    let average_session_duration = metrics.iter()
        .map(|m| m.avg_response_time.unwrap_or(0))
        .sum::<i64>() / metrics.len().max(1) as i64;
    
    let productivity_score = calculate_productivity_score(
        tokens_per_successful_request, 
        request_completion_rate, 
        average_session_duration as f64
    );
    
    let resource_utilization = calculate_resource_utilization(metrics);
    
    EfficiencyMetrics {
        tokens_per_successful_request,
        request_completion_rate,
        average_session_duration,
        productivity_score,
        resource_utilization,
    }
}

/// Analyze usage patterns
fn analyze_usage_patterns(metrics: &[AIUsageMetric]) -> UsagePatterns {
    // Peak usage hours (simplified - would need timestamp analysis)
    let peak_usage_hours = vec![9, 10, 11, 14, 15, 16]; // Typical work hours
    
    // Most used agents
    let mut agent_usage: HashMap<String, i64> = HashMap::new();
    for metric in metrics {
        if let Some(agent) = &metric.agent_type {
            *agent_usage.entry(agent.clone()).or_insert(0) += metric.request_count;
        }
    }
    let mut most_used_agents: Vec<(String, i64)> = agent_usage.into_iter().collect();
    most_used_agents.sort_by(|a, b| b.1.cmp(&a.1));
    let most_used_agents: Vec<String> = most_used_agents.into_iter().take(5).map(|(agent, _)| agent).collect();
    
    // Most used models
    let mut model_usage: HashMap<String, i64> = HashMap::new();
    for metric in metrics {
        *model_usage.entry(metric.model_name.clone()).or_insert(0) += metric.request_count;
    }
    let mut most_used_models: Vec<(String, i64)> = model_usage.into_iter().collect();
    most_used_models.sort_by(|a, b| b.1.cmp(&a.1));
    let most_used_models: Vec<String> = most_used_models.into_iter().take(3).map(|(model, _)| model).collect();
    
    // Session length distribution (simplified)
    let mut session_length_distribution = HashMap::new();
    session_length_distribution.insert("short".to_string(), metrics.len() as i32 / 3);
    session_length_distribution.insert("medium".to_string(), metrics.len() as i32 / 3);
    session_length_distribution.insert("long".to_string(), metrics.len() as i32 / 3);
    
    // Failure patterns
    let failure_patterns = identify_failure_patterns(metrics);
    
    UsagePatterns {
        peak_usage_hours,
        most_used_agents,
        most_used_models,
        session_length_distribution,
        failure_patterns,
    }
}

/// Generate AI usage recommendations
fn generate_ai_recommendations(
    metrics: &[AIUsageMetric], 
    cost_analysis: &CostAnalysis, 
    efficiency: &EfficiencyMetrics
) -> Vec<String> {
    let mut recommendations = Vec::new();
    
    // Cost optimization
    if cost_analysis.cost_per_token > 0.0001 {
        recommendations.push("Consider using more efficient models for simple tasks".to_string());
    }
    
    if cost_analysis.projected_monthly_cost > 100.0 {
        recommendations.push("High projected monthly cost - implement usage monitoring".to_string());
    }
    
    // Efficiency improvements
    if efficiency.tokens_per_successful_request > 5000.0 {
        recommendations.push("Optimize prompts to reduce token usage per request".to_string());
    }
    
    if efficiency.request_completion_rate < 90.0 {
        recommendations.push("Improve error handling to increase success rate".to_string());
    }
    
    // Usage patterns
    let total_failures: i64 = metrics.iter().map(|m| m.failure_count).sum();
    if total_failures > 0 {
        recommendations.push("Investigate and resolve common failure patterns".to_string());
    }
    
    // Model distribution
    let model_count = metrics.iter().map(|m| &m.model_name).collect::<std::collections::HashSet<_>>().len();
    if model_count == 1 {
        recommendations.push("Consider using multiple models for different task types".to_string());
    }
    
    recommendations
}

/// Collect real-time AI data from Claude Code sessions
async fn collect_realtime_ai_data(conn: &Connection, project_id: &str) -> Result<(), String> {
    info!("Collecting real-time AI usage data...");
    
    let current_timestamp = SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .unwrap()
        .as_secs() as i64;
    
    // Simulate collecting data from active Claude Code session
    // In real implementation, this would interface with Claude Code's session data
    
    let session_date = chrono::Utc::now().format("%Y-%m-%d").to_string();
    
    // Example: Current session data
    let current_session = AIUsageMetric {
        id: None,
        project_id: project_id.to_string(),
        model_name: "claude-sonnet-4-20250514".to_string(),
        agent_type: Some("dashboard-analyzer".to_string()),
        mcp_server: Some("sequential".to_string()),
        token_count: 2500,
        request_count: 1,
        success_count: 1,
        failure_count: 0,
        success_rate: Some(100.0),
        avg_response_time: Some(1200),
        total_cost: Some(0.025),
        session_date,
        timestamp: current_timestamp,
    };
    
    // Store current session data
    conn.execute(
        "INSERT OR REPLACE INTO ai_usage_metrics 
         (project_id, model_name, agent_type, mcp_server, token_count, request_count, 
          success_count, failure_count, success_rate, avg_response_time, total_cost, 
          session_date, timestamp) 
         VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8, ?9, ?10, ?11, ?12, ?13)",
        params![
            current_session.project_id,
            current_session.model_name,
            current_session.agent_type,
            current_session.mcp_server,
            current_session.token_count,
            current_session.request_count,
            current_session.success_count,
            current_session.failure_count,
            current_session.success_rate,
            current_session.avg_response_time,
            current_session.total_cost,
            current_session.session_date,
            current_session.timestamp
        ],
    ).map_err(|e| e.to_string())?;
    
    Ok(())
}

/// Helper functions for calculations

fn calculate_cost_efficiency(metrics: &[AIUsageMetric]) -> f64 {
    let total_cost: f64 = metrics.iter().map(|m| m.total_cost.unwrap_or(0.0)).sum();
    let total_successes: i64 = metrics.iter().map(|m| m.success_count).sum();
    
    if total_cost > 0.0 && total_successes > 0 {
        // Higher success per dollar = better efficiency
        (total_successes as f64 / total_cost) * 10.0 // Scale to 0-100
    } else {
        50.0 // Neutral score
    }
}

fn calculate_model_efficiency(success_rate: f64, response_time: i64, tokens: i64, cost: f64) -> f64 {
    let mut score = success_rate; // Base on success rate
    
    // Penalty for slow response
    if response_time > 2000 { score -= 10.0; }
    
    // Bonus for token efficiency
    if tokens > 0 && cost > 0.0 {
        let cost_per_token = cost / tokens as f64;
        if cost_per_token < 0.0001 { score += 10.0; }
    }
    
    score.min(100.0).max(0.0)
}

fn calculate_agent_specialization(agent_type: &str, tokens: i64, success_rate: f64) -> f64 {
    let mut score = success_rate;
    
    // Bonus for specialized agents with good performance
    match agent_type {
        "orchestrator" if success_rate > 90.0 => score += 15.0,
        "frontend" | "backend" | "security" if success_rate > 85.0 => score += 10.0,
        "analyzer" if success_rate > 80.0 => score += 8.0,
        _ => {}
    }
    
    // Bonus for high token usage (indicates active use)
    if tokens > 10000 { score += 5.0; }
    
    score.min(100.0)
}

fn calculate_agent_performance(success_rate: f64, tokens: i64, requests: i64) -> f64 {
    let mut score = success_rate;
    
    // Efficiency bonus
    if requests > 0 {
        let tokens_per_request = tokens as f64 / requests as f64;
        if tokens_per_request < 3000.0 { score += 10.0; } // Efficient token usage
    }
    
    score.min(100.0)
}

fn calculate_mcp_reliability(success_rate: f64, response_time: i64) -> f64 {
    let mut score = success_rate;
    
    // Response time impact
    if response_time < 1000 { score += 10.0; }
    else if response_time > 3000 { score -= 15.0; }
    
    score.min(100.0).max(0.0)
}

fn calculate_mcp_efficiency(server_name: &str, success_rate: f64, requests: i64) -> f64 {
    let mut score = success_rate;
    
    // Server-specific bonuses
    match server_name {
        "context7" if requests > 5 => score += 10.0,
        "sequential" if requests > 3 => score += 15.0,
        "magic" if requests > 2 => score += 8.0,
        "playwright" if requests > 1 => score += 12.0,
        _ => {}
    }
    
    score.min(100.0)
}

fn calculate_productivity_score(tokens_per_request: f64, completion_rate: f64, avg_duration: f64) -> f64 {
    let mut score = completion_rate;
    
    // Token efficiency
    if tokens_per_request < 2000.0 { score += 15.0; }
    else if tokens_per_request > 8000.0 { score -= 10.0; }
    
    // Time efficiency
    if avg_duration < 1500.0 { score += 10.0; }
    else if avg_duration > 4000.0 { score -= 5.0; }
    
    score.min(100.0).max(0.0)
}

fn calculate_resource_utilization(metrics: &[AIUsageMetric]) -> f64 {
    let total_requests: i64 = metrics.iter().map(|m| m.request_count).sum();
    let total_successes: i64 = metrics.iter().map(|m| m.success_count).sum();
    
    if total_requests > 0 {
        (total_successes as f64 / total_requests as f64) * 100.0
    } else {
        0.0
    }
}

fn identify_failure_patterns(metrics: &[AIUsageMetric]) -> Vec<String> {
    let mut patterns = Vec::new();
    
    let total_failures: i64 = metrics.iter().map(|m| m.failure_count).sum();
    if total_failures > 0 {
        patterns.push(format!("Total failures: {}", total_failures));
        
        // Check for specific failure patterns
        let high_failure_models: Vec<&str> = metrics.iter()
            .filter(|m| m.failure_count > m.success_count)
            .map(|m| m.model_name.as_str())
            .collect();
        
        if !high_failure_models.is_empty() {
            patterns.push(format!("High failure models: {:?}", high_failure_models));
        }
    }
    
    patterns
}

fn create_empty_analysis(project_id: String) -> AIUsageAnalysis {
    AIUsageAnalysis {
        project_id,
        total_sessions: 0,
        total_tokens: 0,
        total_requests: 0,
        average_success_rate: 0.0,
        cost_analysis: CostAnalysis {
            total_cost: 0.0,
            daily_average: 0.0,
            cost_per_token: 0.0,
            cost_breakdown_by_model: HashMap::new(),
            projected_monthly_cost: 0.0,
            cost_efficiency_score: 0.0,
        },
        model_breakdown: Vec::new(),
        agent_breakdown: Vec::new(),
        mcp_server_usage: Vec::new(),
        efficiency_metrics: EfficiencyMetrics {
            tokens_per_successful_request: 0.0,
            request_completion_rate: 0.0,
            average_session_duration: 0,
            productivity_score: 0.0,
            resource_utilization: 0.0,
        },
        usage_patterns: UsagePatterns {
            peak_usage_hours: Vec::new(),
            most_used_agents: Vec::new(),
            most_used_models: Vec::new(),
            session_length_distribution: HashMap::new(),
            failure_patterns: Vec::new(),
        },
        recommendations: vec!["No usage data available - start using AI features to generate insights".to_string()],
    }
}

fn get_ai_usage_metrics(
    conn: &Connection,
    project_id: &str,
    days_limit: Option<i64>,
) -> Result<Vec<AIUsageMetric>, String> {
    let query = match days_limit {
        Some(days) => format!(
            "SELECT id, project_id, model_name, agent_type, mcp_server, token_count, 
                    request_count, success_count, failure_count, success_rate, 
                    avg_response_time, total_cost, session_date, timestamp 
             FROM ai_usage_metrics WHERE project_id = ? 
             AND timestamp > (strftime('%s', 'now') - {} * 24 * 60 * 60)
             ORDER BY timestamp DESC",
            days
        ),
        None => "SELECT id, project_id, model_name, agent_type, mcp_server, token_count, 
                        request_count, success_count, failure_count, success_rate, 
                        avg_response_time, total_cost, session_date, timestamp 
                 FROM ai_usage_metrics WHERE project_id = ? 
                 ORDER BY timestamp DESC"
            .to_string(),
    };

    let mut stmt = conn.prepare(&query).map_err(|e| e.to_string())?;
    let metrics = stmt
        .query_map(params![project_id], |row| {
            Ok(AIUsageMetric {
                id: Some(row.get(0)?),
                project_id: row.get(1)?,
                model_name: row.get(2)?,
                agent_type: row.get(3)?,
                mcp_server: row.get(4)?,
                token_count: row.get(5)?,
                request_count: row.get(6)?,
                success_count: row.get(7)?,
                failure_count: row.get(8)?,
                success_rate: row.get(9)?,
                avg_response_time: row.get(10)?,
                total_cost: row.get(11)?,
                session_date: row.get(12)?,
                timestamp: row.get(13)?,
            })
        })
        .map_err(|e| e.to_string())?
        .collect::<rusqlite::Result<Vec<_>>>()
        .map_err(|e| e.to_string())?;

    Ok(metrics)
}